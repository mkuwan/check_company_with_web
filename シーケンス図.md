# 取引先申請情報確認システム シーケンス図

## システム概要
申請された取引先情報（会社名・住所・電話番号等）の真正性・実在性をWebサイト解析により自動で確認し、架空請求やペーパーカンパニーのリスクを低減するシステムのシーケンス図です。

## 1. メインシーケンス図（全体の流れ）

```mermaid
sequenceDiagram
    participant User as ユーザー
    participant Main as main.py
    participant Config as config.py
    participant Utils as utils.py
    participant Analyzer as analyzer.py
    participant Search as search.py
    participant Scraper as scraper.py
    participant Ollama as Ollama API
    participant Google as Google Search API
    participant WebSite as 対象Webサイト
    
    User->>Main: python main.py または TestCompanyInfo
    
    Note over Main: 1. システム初期化
    Main->>Main: load_dotenv()
    Main->>Config: load_config()
    Config->>Config: os.getenv() × 設定項目
    Config-->>Main: config辞書
    Main->>Utils: setup_logger()
    Utils-->>Main: logger設定完了
    Main->>Utils: reset_early_termination()
    
    Note over Main: 2. API使用状況確認
    Main->>Utils: get_current_api_usage()
    Utils-->>Main: current_usage
    Main->>Utils: check_api_usage_warning()
    Utils-->>Main: warning_level
    Main->>Utils: enhanced_check_api_limit()
    Utils-->>Main: can_execute, wait_time
    
    Note over Main: 3. 申請情報取得・整理
    alt テストモード
        Main->>Main: TestCompanyInfo使用
    else コマンドラインモード
        Main->>Main: parse_args()
    end
    Main->>Main: application_info = [company, address, tel, other]
    
    Note over Main: 4. AI検索クエリ生成
    Main->>Analyzer: ai_generate_query(application_info, ollama_url, ollama_model, max_queries)
    Analyzer->>Analyzer: 申請情報解析・プロンプト作成
    Analyzer->>Ollama: POST /api/chat (検索クエリ生成要求)
    Ollama-->>Analyzer: 検索クエリリスト
    Analyzer->>Analyzer: クエリフィルタリング・重複除去
    Analyzer-->>Main: queries = [query1, query2, ...]
    
    Note over Main: 5. 各クエリでの検索・解析ループ
    loop 各検索クエリ
        Main->>Main: check_early_termination()
        alt 早期終了フラグ=True
            Main->>Main: ループ終了
        else 継続
            Main->>Search: google_search(query, api_key, cse_id, num, config)
            Search->>Utils: enhanced_check_api_limit()
            Search->>Utils: record_api_call()
            Search->>Google: GET Custom Search API
            Google-->>Search: 検索結果JSON
            Search->>Utils: update_api_usage()
            Search-->>Main: search_results = [{title, link}, ...]
            
            Note over Main: 6. 各URLでのスクレイピング・解析ループ
            loop 各検索結果URL
                Main->>Main: check_early_termination()
                alt 早期終了フラグ=True
                    Main->>Main: ループ終了
                else 継続
                    Note over Main: 6.1. メインページスクレイピング
                    Main->>Scraper: scrape_page(url, timeout, user_agent)
                    Scraper->>Scraper: check_robots_txt(url, user_agent)
                    Scraper->>WebSite: GET robots.txt
                    WebSite-->>Scraper: robots.txt または 404
                    alt robots.txt許可
                        Scraper->>WebSite: GET ページコンテンツ
                        WebSite-->>Scraper: HTMLコンテンツ
                        Scraper->>Scraper: BeautifulSoup解析(title, content, links)
                        Scraper-->>Main: scraped_result = {title, url, content, links}
                        
                        Note over Main: 6.2. メインページAI解析
                        Main->>Main: process_single_page()
                        Main->>Analyzer: ai_analyze_content(application_info, scraped_result, ollama_url, ollama_model)
                        Analyzer->>Analyzer: 申請情報抽出・プロンプト作成
                        Analyzer->>Ollama: POST /api/chat (一致度解析要求)
                        Ollama-->>Analyzer: JSON{score, reasoning, matched_info, confidence}
                        Analyzer->>Analyzer: スコア正規化(0.0-1.0)
                        Analyzer-->>Main: analysis_result
                        
                        alt スコア >= 閾値(0.95)
                            Main->>Utils: set_early_termination()
                            Main->>Main: 早期終了フラグ設定・処理終了
                        else 継続
                            Note over Main: 6.3. 関連ページスクレイピング
                            Main->>Scraper: scrape_recursive(url, depth=1, max_depth, visited, timeout, user_agent, scrape_interval)
                            loop 関連ページごと
                                Scraper->>Scraper: time.sleep(scrape_interval)
                                Scraper->>Scraper: scrape_page(related_url)
                                Scraper->>Scraper: check_robots_txt(related_url)
                                alt robots.txt許可
                                    Scraper->>WebSite: GET 関連ページコンテンツ
                                    WebSite-->>Scraper: HTMLコンテンツ
                                    Scraper->>Scraper: リンク抽出・同一ドメインフィルタ
                                else robots.txt禁止
                                    Scraper->>Scraper: スキップ
                                end
                            end
                            Scraper-->>Main: scraped_pages = [scraped_result, ...]
                            
                            Note over Main: 6.4. 関連ページAI解析
                            loop 各関連ページ
                                Main->>Main: check_early_termination()
                                alt 早期終了フラグ=True
                                    Main->>Main: ループ終了
                                else 継続
                                    Main->>Main: process_single_page()
                                    Main->>Analyzer: ai_analyze_content()
                                    Analyzer->>Ollama: POST /api/chat
                                    Ollama-->>Analyzer: JSON解析結果
                                    Analyzer-->>Main: analysis_result
                                    
                                    alt スコア >= 閾値(0.95)
                                        Main->>Utils: set_early_termination()
                                        Main->>Main: 早期終了フラグ設定・処理終了
                                    end
                                end
                            end
                        end
                    else robots.txt禁止
                        Scraper-->>Main: error: robots.txt disallowed
                    end
                end
            end
        end
    end
    
    Note over Main: 7. 結果統合・出力
    Main->>Main: 全結果統合・スコア順ソート
    Main->>Main: マッチング判定(best_score >= threshold)
    Main->>Utils: standardize_output_format(raw_result)
    Utils-->>Main: standardized_result
    Main->>Utils: write_result_json(standardized_result)
    Main->>Utils: write_result_markdown(standardized_result)
    Utils->>Utils: ファイル出力(result.json, result.md)
    Main-->>User: 処理完了・判定結果
```

## 2. 初期化・設定シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Env as .env
    participant Config as config.py
    participant Utils as utils.py
    participant OS as os.environ
    
    Main->>Main: 環境変数クリア(OLLAMA_API_URL削除)
    Main->>Env: load_dotenv()
    Env-->>OS: 環境変数読み込み
    
    Main->>Config: load_config()
    Config->>OS: os.getenv("GOOGLE_API_KEY")
    Config->>OS: os.getenv("GOOGLE_CSE_ID")
    Config->>OS: os.getenv("OLLAMA_API_URL")
    Config->>OS: os.getenv("OLLAMA_MODEL")
    Config->>OS: os.getenv("MAX_GOOGLE_SEARCH", 3)
    Config->>OS: os.getenv("GOOGLE_SEARCH_NUM_RESULTS", 3)
    Config->>OS: os.getenv("MAX_SCRAPE_DEPTH", 3)
    Config->>OS: os.getenv("SCORE_THRESHOLD", 0.95)
    Config->>OS: os.getenv("LOG_LEVEL", "INFO")
    Config->>Config: get_int_env()処理
    Config-->>Main: config = {設定値辞書}
    
    Main->>Utils: setup_logger(log_level, log_file)
    Utils->>Utils: logging.basicConfig設定
    Utils->>Utils: FileHandler・StreamHandler設定
    Utils-->>Main: logger設定完了
    
    Main->>Utils: reset_early_termination()
    Utils->>Utils: _early_termination_flag.clear()
    Utils-->>Main: フラグリセット完了
```

## 3. API制限管理シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Utils as utils.py
    participant File as api_log/search_api_count_YYYYMMDD.txt
    participant Time as time.sleep()
    
    Main->>Utils: get_current_api_usage()
    Utils->>File: ファイル読み込み
    File-->>Utils: 本日使用量 or 0
    Utils-->>Main: current_usage
    
    Main->>Utils: check_api_usage_warning(current_usage, daily_limit, config)
    Utils->>Utils: warning_level計算
    Utils-->>Main: warning_level (0=正常, 1=警告, 2=危険)
    
    Main->>Utils: enhanced_check_api_limit(required_calls=max_queries, config)
    Utils->>Utils: 使用量チェック
    Utils->>Utils: レート制限チェック
    alt 制限超過
        Utils-->>Main: can_execute=False, error_msg
        Main->>Main: sys.exit(1) エラー終了
    else 制限内
        Utils->>Utils: 待機時間計算
        alt 待機必要
            Utils-->>Main: can_execute=True, wait_time > 0
            Main->>Time: time.sleep(wait_time)
        else 待機不要
            Utils-->>Main: can_execute=True, wait_time=0
        end
    end
```

## 4. Google検索シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Search as search.py
    participant Utils as utils.py
    participant Google as Google Custom Search API
    participant File as api_log/search_api_count_YYYYMMDD.txt
    participant RateLimit as google_api_rate_limit.json
    
    Main->>Search: google_search(query, api_key, cse_id, num, config)
    
    Search->>Utils: enhanced_check_api_limit(required_calls=1, config)
    Utils->>File: 本日使用量確認
    Utils->>RateLimit: 前回呼出時刻確認
    Utils-->>Search: can_execute, error_msg, wait_time
    
    alt 制限エラー
        Search-->>Main: ValueError("Google Search API制限エラー")
    else 実行可能
        alt 待機必要
            Search->>Search: time.sleep(wait_time)
        end
        
        Search->>Utils: record_api_call(config)
        Utils->>RateLimit: 現在時刻記録
        
        Search->>Google: GET https://www.googleapis.com/customsearch/v1
        Note right of Google: params = {key, cx, q, num}
        Google-->>Search: 検索結果JSON
        
        Search->>Search: 結果解析・URLリスト作成
        Search->>Utils: update_api_usage(config)
        Utils->>File: 本日使用量 +1
        
        Search-->>Main: results = [{title, link}, ...]
    end
```

## 5. スクレイピング・robots.txt遵守シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Scraper as scraper.py
    participant Cache as _robots_cache
    participant RobotParser as RobotFileParser
    participant WebSite as 対象Webサイト
    participant BeautifulSoup as BeautifulSoup
    
    Main->>Scraper: scrape_page(url, timeout, user_agent)
    
    Scraper->>Scraper: check_robots_txt(url, user_agent)
    Scraper->>Scraper: urlparse(url).netloc でドメイン抽出
    
    alt キャッシュ存在
        Scraper->>Cache: キャッシュ確認
        Cache-->>Scraper: cached_robotparser or None
    else キャッシュなし
        Scraper->>RobotParser: RobotFileParser()
        RobotParser->>WebSite: GET https://domain.com/robots.txt
        alt robots.txt取得成功
            WebSite-->>RobotParser: robots.txtコンテンツ
            RobotParser->>RobotParser: read()・parse()
            Scraper->>Cache: robotparser保存
        else robots.txt取得失敗(404等)
            WebSite-->>RobotParser: 404 Not Found
            Scraper->>Cache: None保存
        end
    end
    
    alt robots.txt存在
        Scraper->>RobotParser: can_fetch(user_agent, url)
        RobotParser-->>Scraper: True(許可) or False(禁止)
        alt アクセス禁止
            Scraper-->>Main: {error: "robots.txt disallowed"}
        end
    else robots.txt未存在
        Scraper->>Scraper: 許可として扱う
    end
    
    alt アクセス許可
        Scraper->>WebSite: GET url (headers={'User-Agent': user_agent})
        WebSite-->>Scraper: HTMLコンテンツ
        Scraper->>BeautifulSoup: BeautifulSoup(html, 'html.parser')
        BeautifulSoup->>BeautifulSoup: title抽出
        BeautifulSoup->>BeautifulSoup: script/style除去・content抽出
        BeautifulSoup->>BeautifulSoup: links抽出(urljoin処理)
        Scraper-->>Main: {title, url, content, links}
    end
```

## 6. 再帰スクレイピングシーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Scraper as scraper.py
    participant Time as time.sleep()
    participant WebSite as 対象Webサイト
    
    Main->>Scraper: scrape_recursive(url, depth=1, max_depth, visited, timeout, user_agent, scrape_interval)
    
    Scraper->>Scraper: visited初期化 or 引継ぎ
    Scraper->>Scraper: depth制限チェック
    
    alt depth > max_depth
        Scraper-->>Main: [] 空結果
    else 継続
        alt len(visited) > 1
            Scraper->>Time: time.sleep(scrape_interval)
        end
        
        Scraper->>Scraper: scrape_page(url, timeout, user_agent)
        Note right of Scraper: robots.txt確認・コンテンツ取得
        Scraper->>Scraper: results.append(page)
        Scraper->>Scraper: visited.add(url)
        
        alt depth < max_depth AND コンテンツ取得成功
            Scraper->>Scraper: リンク抽出・同一ドメインフィルタ
            loop 各関連リンク
                alt 未訪問 AND robots.txt許可
                    Scraper->>Scraper: scrape_recursive(link, depth+1, max_depth, visited, timeout, user_agent, scrape_interval)
                    Note right of Scraper: 再帰呼出
                    Scraper->>Scraper: results.extend(recursive_results)
                else 訪問済み or 禁止
                    Scraper->>Scraper: スキップ
                end
            end
        end
        
        Scraper-->>Main: results = [scraped_result, ...]
    end
```

## 7. AI解析シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Analyzer as analyzer.py
    participant Utils as utils.py
    participant Ollama as Ollama API
    
    Main->>Main: process_single_page(application_info, scraped_result, config, search_rank, page_rank, logger)
    
    Main->>Utils: check_early_termination()
    Utils-->>Main: 早期終了フラグ状態
    
    alt 早期終了フラグ=True
        Main-->>Main: None (処理スキップ)
    else 継続
        Main->>Analyzer: ai_analyze_content(application_info, scraped_result, ollama_url, ollama_model)
        
        Analyzer->>Utils: check_early_termination()
        alt 早期終了フラグ=True
            Analyzer-->>Main: EarlyTerminationException
        end
        
        Analyzer->>Analyzer: 申請情報抽出
        Note right of Analyzer: company_name, address, tel, other_info
        Analyzer->>Analyzer: スクレイピング内容要約(最大3000文字)
        Note right of Analyzer: content, title, url
        
        Analyzer->>Analyzer: プロンプト作成
        Note right of Analyzer: 厳密な判定ルール定義<br/>会社名: 完全一致+0.5, 部分一致+0.3<br/>住所: 完全一致+0.25, 部分一致+0.15<br/>電話番号: 一致+0.25
        
        Analyzer->>Ollama: POST /api/chat
        Note right of Ollama: payload = {model, messages, stream=False}
        Ollama-->>Analyzer: JSON response
        
        Analyzer->>Analyzer: レスポンス解析・JSON抽出
        Analyzer->>Analyzer: {から}までの部分抽出
        Analyzer->>Analyzer: json.loads()
        Analyzer->>Analyzer: スコア正規化(0.0-1.0)
        Analyzer->>Analyzer: confidence正規化(0.0-1.0)
        
        Analyzer-->>Main: analysis_result = {score, reasoning, matched_info, confidence, search_rank, page_rank, url, title}
        
        Main->>Main: スコア閾値チェック
        alt score >= score_threshold
            Main->>Utils: set_early_termination()
            Main->>Main: 早期終了フラグ設定・処理継続停止
        end
    end
```

## 8. AI検索クエリ生成シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Analyzer as analyzer.py
    participant Ollama as Ollama API
    
    Main->>Analyzer: ai_generate_query(application_info, ollama_url, ollama_model, max_queries)
    
    Analyzer->>Analyzer: 申請情報解析
    Note right of Analyzer: company_name, address, tel, other_info抽出
    
    Analyzer->>Analyzer: プロンプト作成
    Note right of Analyzer: 検索目的: 企業実在性確認<br/>重要な指針: 対象企業名含む、サンプル除外<br/>生成例: "企業名 概要", "企業名 沿革"等
    
    Analyzer->>Ollama: POST /api/chat
    Note right of Ollama: payload = {model, messages, stream=False}
    Ollama-->>Analyzer: 生成されたクエリテキスト
    
    Analyzer->>Analyzer: レスポンス解析・クエリ抽出
    loop 各行
        Analyzer->>Analyzer: 空行・番号・記号除去
        Analyzer->>Analyzer: 説明文・不要文言除去
        Analyzer->>Analyzer: 会社名フィルタリング
        Analyzer->>Analyzer: サンプル・テスト関連除外
    end
    
    Analyzer->>Analyzer: 重複除去
    Analyzer->>Analyzer: max_queries制限適用
    
    Analyzer-->>Main: unique_queries = [query1, query2, ...]
```

## 9. 早期終了制御シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Utils as utils.py
    participant Flag as _early_termination_flag
    
    Note over Main: システム開始時
    Main->>Utils: reset_early_termination()
    Utils->>Flag: _early_termination_flag.clear()
    
    Note over Main: 処理中の監視
    loop 各処理段階
        Main->>Utils: check_early_termination()
        Utils->>Flag: _early_termination_flag.is_set()
        Flag-->>Utils: True or False
        Utils-->>Main: 早期終了フラグ状態
        
        alt フラグ=True
            Main->>Main: 処理スキップ・ループ終了
        else フラグ=False
            Main->>Main: 処理継続
            
            Note over Main: AI解析結果判定
            Main->>Main: score >= score_threshold チェック
            alt 高スコア検出
                Main->>Utils: set_early_termination()
                Utils->>Flag: _early_termination_flag.set()
                Main->>Main: 高スコア通知・後続処理スキップ設定
            end
        end
    end
```

## 10. 結果出力シーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Utils as utils.py
    participant JSON as result.json
    participant MD as result.md
    participant Logger as app.log
    
    Main->>Main: 全結果統合(all_query_results)
    Main->>Main: スコア順ソート
    Main->>Main: best_score算出・found判定
    
    Main->>Main: raw_result作成
    Note right of Main: {company, address, tel, other, results,<br/>searched_url_count, found, early_terminated}
    
    Main->>Utils: standardize_output_format(raw_result)
    Utils->>Utils: 設計書準拠フォーマット変換
    Utils-->>Main: standardized_result
    
    Main->>Utils: write_result_json(standardized_result)
    Utils->>JSON: JSON形式でファイル出力
    JSON-->>Utils: 出力完了
    
    Main->>Utils: write_result_markdown(standardized_result)
    Utils->>MD: Markdown形式でファイル出力
    MD-->>Utils: 出力完了
    
    Main->>Logger: logger.info() 最終結果ログ
    Logger->>Logger: ログファイル・標準出力
    
    Main->>Main: 完了通知出力
    Main-->>Main: standardized_result返却
```

## 11. エラーハンドリングシーケンス図

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Module as 各モジュール
    participant Utils as utils.py
    participant Logger as logging
    participant Exception as 例外処理
    
    Main->>Module: 各種処理実行
    
    alt 正常処理
        Module-->>Main: 正常結果
    else 例外発生
        Module->>Exception: raise Exception
        
        alt TimeoutException
            Exception->>Main: タイムアウト例外
            Main->>Logger: logger.error("タイムアウト")
            Main->>Main: デフォルト値返却 or 処理継続
            
        else EarlyTerminationException
            Exception->>Main: 早期終了例外
            Main->>Logger: logger.info("早期終了")
            Main->>Main: 処理スキップ・結果出力へ
            
        else API制限エラー
            Exception->>Main: ValueError("API制限エラー")
            Main->>Logger: logger.error("API制限")
            Main->>Main: sys.exit(1) システム終了
            
        else HTTPエラー
            Exception->>Main: requests.HTTPError
            Main->>Logger: logger.error("HTTP error")
            Main->>Main: エラー結果返却・処理継続
            
        else JSON解析エラー
            Exception->>Main: json.JSONDecodeError
            Main->>Logger: logger.error("JSON解析失敗")
            Main->>Main: デフォルト値返却
            
        else robots.txt禁止
            Exception->>Main: error: "robots.txt disallowed"
            Main->>Logger: logger.info("robots.txt skip")
            Main->>Main: 次URLへスキップ
            
        else その他例外
            Exception->>Main: Exception
            Main->>Logger: logger.error("一般例外", exc_info=True)
            Main->>Main: エラー結果返却・処理継続
        end
    end
```

## 12. API レート制限・使用量管理シーケンス図

```mermaid
sequenceDiagram
    participant Search as search.py
    participant Utils as utils.py
    participant UsageFile as api_log/search_api_count_YYYYMMDD.txt
    participant RateFile as google_api_rate_limit.json
    participant Time as datetime.now()
    
    Note over Search: API呼出前チェック
    Search->>Utils: enhanced_check_api_limit(required_calls, config)
    
    Utils->>UsageFile: 本日使用量読み込み
    UsageFile-->>Utils: current_usage or 0
    
    Utils->>Utils: daily_limit制限チェック
    alt 制限超過
        Utils-->>Search: can_execute=False, error_msg
    else 制限内
        Utils->>RateFile: 前回呼出時刻読み込み
        RateFile-->>Utils: last_call_time or None
        
        Utils->>Time: datetime.now()
        Time-->>Utils: current_time
        
        Utils->>Utils: 前回呼出からの経過時間計算
        alt 間隔不足
            Utils->>Utils: 待機時間計算
            Utils-->>Search: can_execute=True, wait_time > 0
        else 間隔十分
            Utils-->>Search: can_execute=True, wait_time=0
        end
    end
    
    Note over Search: API呼出実行
    Search->>Utils: record_api_call(config)
    Utils->>Time: datetime.now()
    Utils->>RateFile: 現在時刻保存
    
    Search->>Search: Google API実行
    
    Note over Search: API呼出後更新
    Search->>Utils: update_api_usage(config)
    Utils->>UsageFile: 使用量 +1
    Utils->>UsageFile: ファイル更新
```

## 重要なシーケンス特徴

### 1. 早期終了制御
- 各処理段階で `check_early_termination()` を実行
- 高スコア検出時に `set_early_termination()` でフラグ設定
- 後続の全処理が自動的にスキップされ、効率的に処理終了

### 2. robots.txt遵守
- 全スクレイピング前に必ず `check_robots_txt()` を実行
- ドメインごとのキャッシュ機能で効率化
- 禁止されたURLは自動的にスキップ

### 3. API制限管理
- Google Search API使用前に制限チェック
- 日次使用量をファイルで永続化
- レート制限による自動待機機能

### 4. エラーハンドリング
- 各段階での例外処理
- ログ出力による詳細な実行記録
- エラー時のデフォルト値返却で処理継続

### 5. 並行処理制御
- 早期終了フラグによる全処理の協調制御
- スレッドセーフな状態管理
- 効率的なリソース使用

このシーケンス図により、システム全体の時系列での動作と各コンポーネント間の相互作用が詳細に把握できます。
