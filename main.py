#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¹ç‡åŒ–ç‰ˆãƒ¡ã‚¤ãƒ³å‡¦ç†

é«˜ã‚¹ã‚³ã‚¢æ¤œå‡ºå¾Œã®ä¸¦è¡Œå‡¦ç†ç¶™ç¶šå•é¡Œã‚’è§£æ±ºã—ã€
é–¢é€£æ€§ã®ä½ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from dotenv import load_dotenv
from config import load_config
from utils import (
    setup_logger, 
    write_result_json, 
    write_result_markdown, 
    get_current_api_usage,
    enhanced_check_api_limit,
    check_api_usage_warning,
    reset_early_termination,
    set_early_termination,
    check_early_termination,
    standardize_output_format
)
import argparse
from analyzer import ai_generate_query
from search import google_search
from scraper import scrape_page, scrape_recursive
import sys
import logging
import time

@dataclass
class TestCompanyInfo:
    """ãƒ†ã‚¹ãƒˆç”¨ä¼æ¥­æƒ…å ±ãƒ¢ãƒ‡ãƒ«"""
    company: str  # ä¼šç¤¾å
    address: str  # ä½æ‰€
    tel: str      # é›»è©±ç•ªå·
    other: Optional[List[str]] = None  # ãã®ä»–æƒ…å ±ï¼ˆæ—§ç¤¾åã€æ”¯åº—åãªã©ï¼‰
    
    def __post_init__(self):
        """åˆæœŸåŒ–å¾Œå‡¦ç†ï¼šotherãŒNoneã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã«è¨­å®š"""
        if self.other is None:
            self.other = []

def parse_args():
    parser = argparse.ArgumentParser(description="å–å¼•å…ˆç”³è«‹æƒ…å ±ã®ç¢ºèªã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument('--company', type=str, required=True, help='ä¼šç¤¾å')
    parser.add_argument('--address', type=str, required=True, help='ä½æ‰€')
    parser.add_argument('--tel', type=str, required=True, help='é›»è©±ç•ªå·')
    parser.add_argument('--other', nargs='*', default=[], help='ãã®ä»–æƒ…å ±ï¼ˆæ—§ç¤¾åã€æ”¯åº—åãªã©ï¼‰')
    return parser.parse_args()

def process_single_page(application_info, scraped_result, config, search_rank, page_rank, logger):
    """å˜ä¸€ãƒšãƒ¼ã‚¸ã®AIè§£æå‡¦ç†ï¼ˆæ—©æœŸçµ‚äº†ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
    # æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ãƒã‚§ãƒƒã‚¯
    if check_early_termination():
        logger.info(f"[{search_rank}-{page_rank}] æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚Šå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—")
        return None
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é–¢é€£æ€§äº‹å‰ãƒã‚§ãƒƒã‚¯
    title = scraped_result.get('title', '')
    url = scraped_result.get('url', '')
    company_name = application_info[0] if len(application_info) > 0 else ""

    
    print(f"[{search_rank}-{page_rank}] AIè§£æé–‹å§‹: {title[:50]}...")
    logger.info(f"[{search_rank}-{page_rank}] AIè§£æé–‹å§‹: {url}")
    
    try:
        from analyzer import ai_analyze_content
        analysis_result = ai_analyze_content(
            application_info,
            scraped_result,
            config["OLLAMA_API_URL"],
            config["OLLAMA_MODEL"]
        )
        
        # è§£æçµæœã‚’è¿½åŠ 
        analysis_result.update({
            "search_rank": search_rank,
            "page_rank": page_rank,
            "url": url,
            "title": title,
            "scraped_content_length": len(scraped_result.get('content', ''))
        })
        score = analysis_result.get("score", 0.0)
        reasoning = analysis_result.get('reasoning', '')
        print(f"[{search_rank}-{page_rank}] AIè§£æå®Œäº†: ã‚¹ã‚³ã‚¢={score:.3f}, åˆ¤å®šç†ç”±={reasoning}")
        logger.info(f"[{search_rank}-{page_rank}] AIè§£æçµæœ: ã‚¹ã‚³ã‚¢={score:.3f}")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"[{search_rank}-{page_rank}] AIè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def main_fixed(test_company_info: Optional[TestCompanyInfo] = None) -> Dict[str, Any]:
    """åŠ¹ç‡åŒ–ç‰ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ—©æœŸçµ‚äº†å•é¡Œã‚’è§£æ±º + äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ï¼‰"""
    # ç’°å¢ƒå¤‰æ•°ã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒªã‚¢ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å›é¿ï¼‰
    if 'OLLAMA_API_URL' in os.environ:
        del os.environ['OLLAMA_API_URL']
    
    load_dotenv()
    config = load_config()
    
    # æ–°ã—ã„ãƒ­ã‚¬ãƒ¼è¨­å®šã‚’é©ç”¨
    logger = setup_logger(
        log_level=config.get('LOG_LEVEL', 'INFO'),
        log_file=config.get('LOG_FILE', 'app.log')
    )
    
    logger.info("=" * 60)
    logger.info("å–å¼•å…ˆç”³è«‹æƒ…å ±ç¢ºèªã‚·ã‚¹ãƒ†ãƒ  é–‹å§‹")
    logger.info("=" * 60)
    
    # æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
    reset_early_termination()
    
    # è¨­å®šå€¤ã®å–å¾—
    max_queries = int(config.get("MAX_GOOGLE_SEARCH", 3))
    num_results = int(config.get("GOOGLE_SEARCH_NUM_RESULTS", 3))
    max_scrape_depth = int(config.get("MAX_SCRAPE_DEPTH", 3))
    score_threshold = float(config.get("SCORE_THRESHOLD", 0.95))

    # APIä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
    current_usage = get_current_api_usage()
    daily_limit = int(config.get('GOOGLE_API_DAILY_LIMIT', '100'))
    warning_level = check_api_usage_warning(current_usage, daily_limit, config)
    
    logger.info(f"æœ¬æ—¥ã®Google Search APIä½¿ç”¨çŠ¶æ³: {current_usage}/{daily_limit}")
    
    # è­¦å‘Šãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    if warning_level == 2:
        print(f"âš ï¸  å±é™º: APIä½¿ç”¨é‡ãŒå±é™ºãƒ¬ãƒ™ãƒ«ã§ã™ ({current_usage}/{daily_limit})")
        logger.warning(f"APIä½¿ç”¨é‡ãŒå±é™ºãƒ¬ãƒ™ãƒ«: {current_usage}/{daily_limit}")
    elif warning_level == 1:
        print(f"âš ï¸  è­¦å‘Š: APIä½¿ç”¨é‡ãŒè­¦å‘Šãƒ¬ãƒ™ãƒ«ã§ã™ ({current_usage}/{daily_limit})")
        logger.warning(f"APIä½¿ç”¨é‡ãŒè­¦å‘Šãƒ¬ãƒ™ãƒ«: {current_usage}/{daily_limit}")
    
    # å¼·åŒ–ã•ã‚ŒãŸAPIåˆ¶é™ãƒã‚§ãƒƒã‚¯
    can_execute, error_msg, wait_time = enhanced_check_api_limit(required_calls=max_queries, config=config)
    if not can_execute:
        print(f"âŒ APIåˆ¶é™ã‚¨ãƒ©ãƒ¼: {error_msg}")
        logger.error(f"APIåˆ¶é™ã«ã‚ˆã‚Šå®Ÿè¡Œåœæ­¢: {error_msg}")
        sys.exit(1)
    
    if wait_time > 0:
        print(f"â±ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«ã‚ˆã‚Š{wait_time:.1f}ç§’å¾…æ©Ÿã—ã¾ã™...")
        time.sleep(wait_time)
      # ä¼æ¥­æƒ…å ±ã®å–å¾—
    if test_company_info:
        company = test_company_info.company
        address = test_company_info.address
        tel = test_company_info.tel
        other = test_company_info.other or []
        logger.info("ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")
    else:
        args = parse_args()
        company = args.company
        address = args.address
        tel = args.tel
        other = args.other
        logger.info("ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")
    
    logger.info(f"å—ã‘å–ã£ãŸç”³è«‹æƒ…å ±: ä¼šç¤¾å={company}, ä½æ‰€={address}, é›»è©±ç•ªå·={tel}, ãã®ä»–={other}")
    application_info = [company, address, tel] + other
    
    print("å—ã‘å–ã£ãŸç”³è«‹æƒ…å ±:")
    print(f"ä¼šç¤¾å: {company}")
    print(f"ä½æ‰€: {address}")
    print(f"é›»è©±ç•ªå·: {tel}")
    print(f"ãã®ä»–: {other}")
    print(f"æœ¬æ—¥ã®APIä½¿ç”¨çŠ¶æ³: {current_usage}/{daily_limit}")
    
    # å…¨çµæœã‚’è“„ç©ã™ã‚‹ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
    all_query_results = []
    total_searched_urls = 0
    overall_found_match = False
    
    # AIã«ã‚ˆã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
    try:
        queries = ai_generate_query(
            application_info,
            config["OLLAMA_API_URL"],
            config["OLLAMA_MODEL"],
            max_queries=max_queries
        )
        logger.info(f"AIç”Ÿæˆæ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ: {queries}")
        print(f"AIç”Ÿæˆæ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ: {queries}")
    except Exception as e:
        logger.error(f"AIã«ã‚ˆã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆã«å¤±æ•—: {e}", exc_info=True)
        print("AIã«ã‚ˆã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
      # å„ã‚¯ã‚¨ãƒªã”ã¨ã«Googleæ¤œç´¢ã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»AIè§£æ
    for idx, query in enumerate(queries, 1):
        if check_early_termination():
            logger.info(f"æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚Šã‚¯ã‚¨ãƒª{idx}ä»¥é™ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            break
        logger.info(f"[{idx}] æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        print(f"[{idx}] æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")
        try:
            # Googleæ¤œç´¢å®Ÿè¡Œ
            search_results = google_search(
                query,
                config["GOOGLE_API_KEY"],
                config["GOOGLE_CSE_ID"],
                num=num_results,
                config=config
            )
            logger.info(f"Googleæ¤œç´¢çµæœä»¶æ•°: {len(search_results)}ä»¶")
            print(f"Googleæ¤œç´¢çµæœ: {len(search_results)}ä»¶")
            
            for i, item in enumerate(search_results, 1):
                logger.info(f"[{i}] {item['title']} {item['link']}")
                print(f"[{i}] {item['title']} {item['link']}")
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»AIè§£æ
            all_analysis_results = []
            found_match = False
            
            for i, item in enumerate(search_results, 1):
                if check_early_termination() or found_match:
                    logger.info(f"æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ã¾ãŸã¯é«˜ã‚¹ã‚³ã‚¢æ¤œå‡ºã«ã‚ˆã‚Šæ¤œç´¢{i}ä»¥é™ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    break
                
                print(f"\n[{i}] ãƒšãƒ¼ã‚¸è§£æé–‹å§‹: {item['title']}")
                logger.info(f"[{i}] ãƒšãƒ¼ã‚¸è§£æé–‹å§‹: {item['link']}")
                
                try:                    # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»AIè§£æ
                    print(f"[{i}] ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {item['link']}")
                    logger.info(f"[{i}] ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {item['link']}")
                    
                    user_agent = config.get("SCRAPER_USER_AGENT", "Mozilla/5.0 (compatible; CompanyVerificationBot/1.0)")
                    main_scraped = scrape_page(item['link'], timeout=10, user_agent=user_agent)
                    if main_scraped and 'error' not in main_scraped:
                        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®AIè§£æ
                        total_searched_urls += 1
                        main_analysis = process_single_page(
                            application_info, main_scraped, config, i, 0, logger
                        )
                        
                        if main_analysis:
                            all_analysis_results.append(main_analysis)
                            main_score = main_analysis.get("score", 0.0)
                            
                            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§é–¾å€¤ãƒã‚§ãƒƒã‚¯
                            if main_score >= score_threshold:
                                print(f"\nâ˜…â˜…â˜… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§é«˜ã‚¹ã‚³ã‚¢æ¤œå‡º! (ã‚¹ã‚³ã‚¢={main_score:.3f} >= {score_threshold}) â˜…â˜…â˜…")
                                logger.info(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§é«˜ã‚¹ã‚³ã‚¢æ¤œå‡ºã«ã‚ˆã‚Šå‡¦ç†æ—©æœŸçµ‚äº†: ã‚¹ã‚³ã‚¢={main_score:.3f}")
                                set_early_termination()
                                found_match = True
                                break
                          # æ—©æœŸçµ‚äº†ã—ã¦ã„ãªã„å ´åˆã®ã¿é–¢é€£ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                        if not check_early_termination() and not found_match:
                            print(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
                            logger.info(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {item['link']}")
                            
                            user_agent = config.get("SCRAPER_USER_AGENT", "Mozilla/5.0 (compatible; CompanyVerificationBot/1.0)")
                            scrape_interval = float(config.get("SCRAPER_INTERVAL", 1.0))
                            
                            scraped_pages = scrape_recursive(
                                item['link'], 
                                depth=1, 
                                max_depth=max_scrape_depth,
                                timeout=10,
                                user_agent=user_agent,
                                scrape_interval=scrape_interval
                            )
                    else:
                        print(f"[{i}] ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—")
                        logger.warning(f"[{i}] ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {item['link']}")
                        continue  # æ¬¡ã®URLã¸
                        
                    # é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®å‡¦ç†
                    if not scraped_pages:
                        print(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœãªã—")
                        logger.warning(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœãªã—: {item['link']}")
                    else:
                        print(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(scraped_pages)}ãƒšãƒ¼ã‚¸")
                        logger.info(f"[{i}] é–¢é€£ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(scraped_pages)}ãƒšãƒ¼ã‚¸")
                        
                        # é–¢é€£ãƒšãƒ¼ã‚¸ã®AIè§£æ                        
                        for page_idx, scraped_result in enumerate(scraped_pages, 1):
                            if check_early_termination():
                                logger.info(f"[{i}-{page_idx}] æ—©æœŸçµ‚äº†ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚Šæ®‹ã‚Šã®ãƒšãƒ¼ã‚¸è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—")
                                break
                                
                            if 'error' in scraped_result:
                                error_msg = scraped_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
                                if error_msg == 'robots.txt disallowed':
                                    print(f"[{i}-{page_idx}] robots.txtã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {scraped_result.get('url', '')}")
                                    logger.info(f"[{i}-{page_idx}] robots.txtã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {scraped_result.get('url', '')}")
                                else:
                                    print(f"[{i}-{page_idx}] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {error_msg}")
                                    logger.warning(f"[{i}-{page_idx}] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {scraped_result.get('url', '')} - {error_msg}")
                                continue
                            
                            # é–¢é€£ãƒšãƒ¼ã‚¸ã®AIè§£æ
                            total_searched_urls += 1
                            analysis_result = process_single_page(
                                application_info, scraped_result, config, i, page_idx, logger
                            )
                            if analysis_result:
                                all_analysis_results.append(analysis_result)
                                score = analysis_result.get("score", 0.0)
                                
                                # é–¢é€£ãƒšãƒ¼ã‚¸ã§ã®é–¾å€¤ãƒã‚§ãƒƒã‚¯
                                if score >= score_threshold:
                                    print(f"\nâ˜…â˜…â˜… é–¢é€£ãƒšãƒ¼ã‚¸ã§é«˜ã‚¹ã‚³ã‚¢æ¤œå‡º! (ã‚¹ã‚³ã‚¢={score:.3f} >= {score_threshold}) â˜…â˜…â˜…")
                                    logger.info(f"é–¢é€£ãƒšãƒ¼ã‚¸ã§é«˜ã‚¹ã‚³ã‚¢æ¤œå‡ºã«ã‚ˆã‚Šå‡¦ç†æ—©æœŸçµ‚äº†: ã‚¹ã‚³ã‚¢={score:.3f}")
                                    set_early_termination()
                                    found_match = True
                                    break
                    
                    # 4. ç¾åœ¨ã®URLã®è§£æçµæœçµ±è¨ˆã‚’è¡¨ç¤º
                    current_url_results = [r for r in all_analysis_results if r.get("search_rank") == i]
                    if current_url_results:
                        max_score = max(r.get("score", 0.0) for r in current_url_results)
                        print(f"[{i}] ãƒšãƒ¼ã‚¸è§£æå®Œäº†: è§£æä»¶æ•°={len(current_url_results)}, æœ€é«˜ã‚¹ã‚³ã‚¢={max_score:.3f}")
                        logger.info(f"[{i}] ãƒšãƒ¼ã‚¸è§£æçµæœ: è§£æä»¶æ•°={len(current_url_results)}, æœ€é«˜ã‚¹ã‚³ã‚¢={max_score:.3f}")
                    else:
                        print(f"[{i}] ãƒšãƒ¼ã‚¸è§£æå®Œäº†: æœ‰åŠ¹ãªè§£æçµæœãªã—")
                        logger.info(f"[{i}] ãƒšãƒ¼ã‚¸è§£æçµæœ: æœ‰åŠ¹ãªè§£æçµæœãªã—")
                        
                    if found_match:
                        break
                    
                except Exception as e:
                    print(f"[{i}] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°/è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
                    logger.error(f"[{i}] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°/è§£æã‚¨ãƒ©ãƒ¼: {item['link']} {e}", exc_info=True)
            
            # ã‚¯ã‚¨ãƒªçµæœã®è¡¨ç¤ºã¨è“„ç©
            print(f"\nã‚¯ã‚¨ãƒª[{idx}]ã®è§£æçµæœ: {len(all_analysis_results)}ä»¶")
            all_analysis_results.sort(key=lambda x: x.get("score", 0.0), reverse=True)
            for i, result in enumerate(all_analysis_results, 1):
                print(f"  [{i}] ã‚¹ã‚³ã‚¢={result.get('score', 0.0):.3f} - {result.get('title', '')[:50]} - {result.get('url', '')}")
            
            # çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒªã‚¹ãƒˆã«è¿½åŠ 
            all_query_results.extend(all_analysis_results)
            # total_searched_urls += len(search_results)
            
            # é«˜ã‚¹ã‚³ã‚¢ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯å…¨ä½“ã®ã‚¯ã‚¨ãƒªå‡¦ç†ã‚‚çµ‚äº†
            if found_match:
                overall_found_match = True
                logger.info(f"é«˜ã‚¹ã‚³ã‚¢æ¤œå‡ºã«ã‚ˆã‚Šå…¨ã‚¯ã‚¨ãƒªå‡¦ç†ã‚’æ—©æœŸçµ‚äº†")
                break
                
        except Exception as e:
            logger.error(f"Googleæ¤œç´¢APIã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            print("Googleæ¤œç´¢APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    # å…¨ã‚¯ã‚¨ãƒªã‹ã‚‰ã®ã™ã¹ã¦ã®çµæœã‚’çµ±åˆã—ã€ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    all_query_results.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    
    # ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®šï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ã§åˆ¤å®šï¼‰
    best_score = all_query_results[0].get("score", 0.0) if all_query_results else 0.0
    found = best_score >= score_threshold
    
    # åˆ¤å®šçµæœã®JSON/Markdownå‡ºåŠ›ï¼ˆæ¨™æº–åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨ï¼‰
    raw_result = {
        "company": company,
        "address": address,
        "tel": tel,
        "other": other,
        "results": all_query_results,
        "searched_url_count": total_searched_urls,
        "found": found,
        "early_terminated": overall_found_match
    }
    
    # è¨­è¨ˆæ›¸æº–æ‹ ã®æ¨™æº–åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    standardized_result = standardize_output_format(raw_result)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    write_result_json(standardized_result)
    write_result_markdown(standardized_result)
    
    # ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"åˆ¤å®šçµæœå‡ºåŠ›å®Œäº†: found={standardized_result['found']}, searched_urls={standardized_result['searched_url_count']}, early_terminated={standardized_result['early_terminated']}")
    print(f"âœ… åˆ¤å®šçµæœã‚’result.jsonã¨result.mdã«å‡ºåŠ›ã—ã¾ã—ãŸ")
    print(f"ğŸ“Š æœ€çµ‚çµæœ: found={standardized_result['found']}, URLs={standardized_result['searched_url_count']}, æ—©æœŸçµ‚äº†={standardized_result['early_terminated']}")
    
    return standardized_result

def main(test_company_info: Optional[TestCompanyInfo] = None) -> Dict[str, Any]:
    return main_fixed(test_company_info)

if __name__ == "__main__":
    
    testCompany01: TestCompanyInfo = TestCompanyInfo(
        company="ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾",
        address="æ„›çŸ¥çœŒè±Šç”°å¸‚ãƒˆãƒ¨ã‚¿ç”º1ç•ªåœ°",
        tel="0565-28-2121",
        # other=["æ—§ç¤¾å: ãƒ†ã‚¹ãƒˆå•†äº‹", "æ”¯åº—å: æ–°å®¿æ”¯åº—"]
    )

    main(testCompany01)
